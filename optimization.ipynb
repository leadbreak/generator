{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from torch import quantize_per_tensor\n",
    "\n",
    "bert_ckpt = \"transformersbook/distilbert-base-uncased-finetuned-clinc\"\n",
    "pipe = pipeline(\"text-classification\", model=bert_ckpt)\n",
    "state_dict = pipe.model.state_dict()\n",
    "state_dict.keys()\n",
    "\n",
    "weights = state_dict[\"distilbert.transformer.layer.0.attention.out_lin.weight\"]\n",
    "scale = (weights.max() - weights.min()) / 255\n",
    "zero_point = 0\n",
    "dtype = torch.qint8\n",
    "quantized_weights = quantize_per_tensor(weights, scale, zero_point, dtype)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "ax1.set_title('before quantization')\n",
    "ax2.set_title('after quantization')\n",
    "\n",
    "ax1.hist(weights.flatten().numpy(), bins=250, range=(-0.3, 0.3))\n",
    "ax2.hist(quantized_weights.flatten().dequantize().numpy(), bins=250, range=(-0.3, 0.3))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = state_dict[\"distilbert.transformer.layer.0.attention.out_lin.weight\"]\n",
    "scale = (weights.max() - weights.min()) / 255\n",
    "zero_point = 0\n",
    "dtype = torch.qint8\n",
    "quantized_weights = quantize_per_tensor(weights, scale, zero_point, dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "ax1.set_title('before quantization')\n",
    "ax2.set_title('after quantization')\n",
    "\n",
    "ax1.hist(weights.flatten().numpy(), bins=250, range=(-0.3, 0.3))\n",
    "ax2.hist(quantized_weights.flatten().dequantize().numpy(), bins=250, range=(-0.3, 0.3))\n",
    "# plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -r 10 -n 1000\n",
    "weights @ weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.quantized import QFunctional\n",
    "\n",
    "q_fn = QFunctional()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -r 10 -n 1000\n",
    "q_fn.mul(quantized_weights, quantized_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.quantization import quantize_dynamic\n",
    "from torch import nn\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model_ckpt = 'transformersbook/bert-base-uncased-finetuned-clinc'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_ckpt).to('cpu')\n",
    "model_quantized = quantize_dynamic(model, {nn.Linear}, dtype=torch.qint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "from pathlib import Path\n",
    "from time import perf_counter\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "\n",
    "clinc = load_dataset(\"clinc_oos\", \"plus\")\n",
    "intents = clinc[\"test\"].features[\"intent\"]\n",
    "accuracy_score = load_metric(\"accuracy\")\n",
    "\n",
    "class PerformanceBenchmark :\n",
    "    def __init__(self, pipeline, dataset, optim_type=\"BERT baseline\") :\n",
    "        self.pipeline = pipeline\n",
    "        self.dataset = dataset\n",
    "        self.optim_type = optim_type\n",
    "        \n",
    "    def compute_accuracy(self) :\n",
    "        preds, labels = [], []\n",
    "        for example in self.dataset :\n",
    "            pred = self.pipeline(example[\"text\"])[0][\"label\"]\n",
    "            label = example[\"intent\"]\n",
    "            preds.append(intents.str2int(pred))\n",
    "            labels.append(label)\n",
    "        accuracy = accuracy_score.compute(predictions=preds, references=labels)\n",
    "        print(f\"valid accuracy : {accuracy['accuracy']:.4f}\")\n",
    "        return accuracy\n",
    "    \n",
    "    def compute_size(self) :\n",
    "        state_dict = self.pipeline.model.state_dict()\n",
    "        tmp_path = Path(\"model.pt\")\n",
    "        torch.save(state_dict, tmp_path)\n",
    "        size_mb = Path(tmp_path).stat().st_size / (1024*1024)\n",
    "        tmp_path.unlink()\n",
    "        print(f\"model size : {size_mb:.4f} MB\")\n",
    "        return {\"size_mb\": size_mb}\n",
    "            \n",
    "    def time_pipeline(self, query=\"What is the pin number for my account?\") :\n",
    "        latencies = []\n",
    "        for _ in range(10) :\n",
    "            _ = self.pipeline(query)\n",
    "        for _ in range(100) :\n",
    "            start_time = perf_counter()\n",
    "            _ = self.pipeline(query)\n",
    "            latency = perf_counter() - start_time\n",
    "            latencies.append(latency)\n",
    "        time_avg_ms = 1000 * np.mean(latencies)\n",
    "        time_std_ms = 1000 * np.std(latencies)\n",
    "        print(f\"time avg : {time_avg_ms:.4f} ms +\\- {time_std_ms:.4f} ms\")\n",
    "        return {\"time_avg_ms\": time_avg_ms, \"time_std_ms\": time_std_ms}    \n",
    "    \n",
    "    def run_benchmark(self) :\n",
    "        metrics = {}\n",
    "        metrics[self.optim_type] = self.compute_size()\n",
    "        metrics[self.optim_type].update(self.time_pipeline())\n",
    "        metrics[self.optim_type].update(self.compute_accuracy())\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original\n",
    "pipe = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "optim_type = \"normal\"\n",
    "pb = PerformanceBenchmark(pipe, clinc[\"test\"], optim_type)\n",
    "perf_metrics = pb.run_benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantized\n",
    "pipe = pipeline(\"text-classification\", model=model_quantized, tokenizer=tokenizer)\n",
    "optim_type = \"quantization\"\n",
    "pb = PerformanceBenchmark(pipe, clinc[\"test\"], optim_type)\n",
    "perf_metrics.update(pb.run_benchmark())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "class DistilTrainingArguments(TrainingArguments):\n",
    "    def __init__(self, *args, alpha=0.5, temperature=2.0, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature\n",
    "        \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import Trainer\n",
    "\n",
    "class DistilTrainer(Trainer):\n",
    "    def __init__(self, *args, teacher=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.teacher = teacher\n",
    "        \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # student의 예측 결과\n",
    "        outputStudents = model(**inputs)\n",
    "        \n",
    "        # student의 loss와 logits        \n",
    "        loss_ce = outputStudents.loss\n",
    "        logit_stu = outputStudents.logits\n",
    "        \n",
    "        # teacher의 logits\n",
    "        with torch.no_grad():\n",
    "            ouputTeacher = self.teacher(**inputs)\n",
    "            logit_tea = ouputTeacher.logits\n",
    "            \n",
    "        # Gumbel-Softmax\n",
    "        loss_fct = nn.KLDivLoss(reduction='batchmean')\n",
    "        temperature = self.args.temperature\n",
    "        loss_kd = temperature**2 * loss_fct(F.log_softmax(logit_stu/temperature , dim=-1), F.softmax(logit_tea/temperature, dim=-1))\n",
    "        \n",
    "        # return averaged student loss\n",
    "        loss = self.args.alpha * loss_ce + (1 - self.args.alpha) * loss_kd\n",
    "        \n",
    "        return (loss, outputStudents) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "clinc = load_dataset(\"clinc_oos\", \"plus\")\n",
    "intents = clinc[\"test\"].features[\"intent\"]\n",
    "num_labels = intents.num_classes\n",
    "\n",
    "accuracy_score = load_metric(\"accuracy\")\n",
    "\n",
    "student_ckpt = \"distilbert-base-uncased\"\n",
    "teacher_ckpt = \"bert-base-uncased\"\n",
    "\n",
    "student = (AutoModelForSequenceClassification.from_pretrained(student_ckpt, num_labels=num_labels).to(device))\n",
    "teacher = (AutoModelForSequenceClassification.from_pretrained(teacher_ckpt, num_labels=num_labels).to(device))\n",
    "\n",
    "print(f\"teacher 대비 student의 파라미터 비율: {student.num_parameters() / teacher.num_parameters() * 100:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset, load_metric\n",
    "import numpy as np\n",
    "\n",
    "clinc = load_dataset(\"clinc_oos\", \"plus\")\n",
    "intents = clinc[\"test\"].features[\"intent\"]\n",
    "accuracy_score = load_metric(\"accuracy\")\n",
    "\n",
    "student_ckpt = \"distilbert-base-uncased\"\n",
    "student_tokenizer = AutoTokenizer.from_pretrained(student_ckpt)\n",
    "\n",
    "def tokenize_text(batch):\n",
    "    return student_tokenizer(batch[\"text\"], truncation=True)\n",
    "\n",
    "clinc_enc = clinc.map(tokenize_text, batched=True, remove_columns=['text'])\n",
    "clinc_enc = clinc_enc.rename_column('intent', 'labels')\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    preds, labels = pred\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    return accuracy_score.compute(predictions=preds, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "finetuned_ckpt = \"distilbert-base-uncased-finetuned-clinc\"\n",
    "student_training_args = DistilTrainingArguments(output_dir=finetuned_ckpt, \n",
    "                                                 evaluation_strategy='epoch', \n",
    "                                                 num_train_epochs=100,\n",
    "                                                 logging_steps=100,\n",
    "                                                 learning_rate=2e-5, \n",
    "                                                 per_device_train_batch_size=batch_size, \n",
    "                                                 per_device_eval_batch_size=batch_size, \n",
    "                                                 alpha=1, \n",
    "                                                 temperature=2,\n",
    "                                                 weight_decay=0.01, \n",
    "                                                 push_to_hub=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoConfig, AutoModelForSequenceClassification\n",
    "\n",
    "bert_ckpt = \"transformersbook/distilbert-base-uncased-finetuned-clinc\"\n",
    "pipe = pipeline(\"text-classification\", model=bert_ckpt)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "id2label = pipe.model.config.id2label\n",
    "label2id = pipe.model.config.label2id\n",
    "\n",
    "num_labels = intents.num_classes\n",
    "student_config = (AutoConfig.from_pretrained(student_ckpt, num_labels=num_labels, id2label=id2label, label2id=label2id))\n",
    "\n",
    "def student_init():\n",
    "    return (AutoModelForSequenceClassification.from_pretrained(student_ckpt, config=student_config).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_ckpt = \"transformersbook/bert-base-uncased-finetuned-clinc\"\n",
    "teacher = (AutoModelForSequenceClassification.from_pretrained(teacher_ckpt, num_labels=num_labels).to(device))\n",
    "\n",
    "distilbert_trainer = DistilTrainer(model_init=student_init, \n",
    "                                    teacher=teacher, \n",
    "                                    args=student_training_args, \n",
    "                                    train_dataset=clinc_enc['train'],\n",
    "                                    eval_dataset=clinc_enc['validation'],\n",
    "                                    compute_metrics=compute_metrics,\n",
    "                                    tokenizer=student_tokenizer)\n",
    "\n",
    "distilbert_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "\n",
    "finetuned_ckpt = \"distilbert-base-uncased-finetuned-clinc\"\n",
    "student_training_args = DistilTrainingArguments(output_dir=finetuned_ckpt, \n",
    "                                                 evaluation_strategy='epoch', \n",
    "                                                 num_train_epochs=100,\n",
    "                                                 logging_steps=100,\n",
    "                                                 learning_rate=2e-5, \n",
    "                                                 per_device_train_batch_size=batch_size, \n",
    "                                                 per_device_eval_batch_size=batch_size, \n",
    "                                                 alpha=0.7, \n",
    "                                                 temperature=2,\n",
    "                                                 weight_decay=0.01, \n",
    "                                                 push_to_hub=False)\n",
    "\n",
    "teacher_ckpt = \"transformersbook/bert-base-uncased-finetuned-clinc\"\n",
    "teacher = (AutoModelForSequenceClassification.from_pretrained(teacher_ckpt, num_labels=num_labels).to(device))\n",
    "\n",
    "distilbert_trainer = DistilTrainer(model_init=student_init, \n",
    "                                    teacher=teacher, \n",
    "                                    args=student_training_args, \n",
    "                                    train_dataset=clinc_enc['train'],\n",
    "                                    eval_dataset=clinc_enc['validation'],\n",
    "                                    compute_metrics=compute_metrics,\n",
    "                                    tokenizer=student_tokenizer)\n",
    "\n",
    "distilbert_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "\n",
    "finetuned_ckpt = \"distilbert-base-uncased-finetuned-clinc\"\n",
    "student_training_args = DistilTrainingArguments(output_dir=finetuned_ckpt, \n",
    "                                                 evaluation_strategy='epoch', \n",
    "                                                 num_train_epochs=100,\n",
    "                                                 logging_steps=100,\n",
    "                                                 learning_rate=2e-5, \n",
    "                                                 per_device_train_batch_size=batch_size, \n",
    "                                                 per_device_eval_batch_size=batch_size, \n",
    "                                                 alpha=0.7, \n",
    "                                                 temperature=10,\n",
    "                                                 weight_decay=0.01, \n",
    "                                                 push_to_hub=False)\n",
    "\n",
    "teacher_ckpt = \"transformersbook/bert-base-uncased-finetuned-clinc\"\n",
    "teacher = (AutoModelForSequenceClassification.from_pretrained(teacher_ckpt, num_labels=num_labels).to(device))\n",
    "\n",
    "distilbert_trainer = DistilTrainer(model_init=student_init, \n",
    "                                    teacher=teacher, \n",
    "                                    args=student_training_args, \n",
    "                                    train_dataset=clinc_enc['train'],\n",
    "                                    eval_dataset=clinc_enc['validation'],\n",
    "                                    compute_metrics=compute_metrics,\n",
    "                                    tokenizer=student_tokenizer)\n",
    "\n",
    "distilbert_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "\n",
    "finetuned_ckpt = \"distilbert-base-uncased-finetuned-clinc\"\n",
    "student_training_args = DistilTrainingArguments(output_dir=finetuned_ckpt, \n",
    "                                                 evaluation_strategy='epoch', \n",
    "                                                 num_train_epochs=200,\n",
    "                                                 logging_steps=100,\n",
    "                                                 learning_rate=2e-5, \n",
    "                                                 per_device_train_batch_size=batch_size, \n",
    "                                                 per_device_eval_batch_size=batch_size, \n",
    "                                                 alpha=0.5, \n",
    "                                                 temperature=5,\n",
    "                                                 weight_decay=0.01, \n",
    "                                                 push_to_hub=False)\n",
    "\n",
    "teacher_ckpt = \"transformersbook/bert-base-uncased-finetuned-clinc\"\n",
    "teacher = (AutoModelForSequenceClassification.from_pretrained(teacher_ckpt, num_labels=num_labels).to(device))\n",
    "\n",
    "distilbert_trainer = DistilTrainer(model_init=student_init, \n",
    "                                    teacher=teacher, \n",
    "                                    args=student_training_args, \n",
    "                                    train_dataset=clinc_enc['train'],\n",
    "                                    eval_dataset=clinc_enc['validation'],\n",
    "                                    compute_metrics=compute_metrics,\n",
    "                                    tokenizer=student_tokenizer)\n",
    "\n",
    "distilbert_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "from pathlib import Path\n",
    "from time import perf_counter\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "\n",
    "clinc = load_dataset(\"clinc_oos\", \"plus\")\n",
    "intents = clinc[\"test\"].features[\"intent\"]\n",
    "accuracy_score = load_metric(\"accuracy\")\n",
    "\n",
    "class PerformanceBenchmark :\n",
    "    def __init__(self, pipeline, dataset, optim_type=\"BERT baseline\") :\n",
    "        self.pipeline = pipeline\n",
    "        self.dataset = dataset\n",
    "        self.optim_type = optim_type\n",
    "        \n",
    "    def compute_accuracy(self) :\n",
    "        preds, labels = [], []\n",
    "        for example in self.dataset :\n",
    "            pred = self.pipeline(example[\"text\"])[0][\"label\"]\n",
    "            label = example[\"intent\"]\n",
    "            preds.append(intents.str2int(pred))\n",
    "            labels.append(label)\n",
    "        accuracy = accuracy_score.compute(predictions=preds, references=labels)\n",
    "        print(f\"valid accuracy : {accuracy['accuracy']:.4f}\")\n",
    "        return accuracy\n",
    "    \n",
    "    def compute_size(self) :\n",
    "        state_dict = self.pipeline.model.state_dict()\n",
    "        tmp_path = Path(\"model.pt\")\n",
    "        torch.save(state_dict, tmp_path)\n",
    "        size_mb = Path(tmp_path).stat().st_size / (1024*1024)\n",
    "        tmp_path.unlink()\n",
    "        print(f\"model size : {size_mb:.4f} MB\")\n",
    "        return {\"size_mb\": size_mb}\n",
    "            \n",
    "    def time_pipeline(self, query=\"What is the pin number for my account?\") :\n",
    "        latencies = []\n",
    "        for _ in range(10) :\n",
    "            _ = self.pipeline(query)\n",
    "        for _ in range(100) :\n",
    "            start_time = perf_counter()\n",
    "            _ = self.pipeline(query)\n",
    "            latency = perf_counter() - start_time\n",
    "            latencies.append(latency)\n",
    "        time_avg_ms = 1000 * np.mean(latencies)\n",
    "        time_std_ms = 1000 * np.std(latencies)\n",
    "        print(f\"time avg : {time_avg_ms:.4f} ms +\\- {time_std_ms:.4f} ms\")\n",
    "        return {\"time_avg_ms\": time_avg_ms, \"time_std_ms\": time_std_ms}    \n",
    "    \n",
    "    def run_benchmark(self) :\n",
    "        metrics = {}\n",
    "        metrics[self.optim_type] = self.compute_size()\n",
    "        metrics[self.optim_type].update(self.time_pipeline())\n",
    "        metrics[self.optim_type].update(self.compute_accuracy())\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "\n",
    "# Original\n",
    "model_ckpt = 'transformersbook/bert-base-uncased-finetuned-clinc'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_ckpt).to('cpu')\n",
    "\n",
    "pipe = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "optim_type = \"original-BERT\"\n",
    "# pb = PerformanceBenchmark(pipe, clinc[\"validation\"], optim_type)\n",
    "# perf_metrics = pb.run_benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distilation Model & tokenizer\n",
    "\n",
    "model_ckpt = './distilbert-base-uncased-finetuned-clinc/checkpoint-500'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_ckpt).to('cpu')\n",
    "\n",
    "pipe = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "optim_type = \"distilBert\"\n",
    "# pb = PerformanceBenchmark(pipe, clinc[\"test\"], optim_type)\n",
    "# perf_metrics = pb.run_benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Original Model & Tokenizer\n",
    "model_ckpt = 'transformersbook/bert-base-uncased-finetuned-clinc'\n",
    "tokenizer_original = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model_original = AutoModelForSequenceClassification.from_pretrained(model_ckpt).to('cpu')\n",
    "\n",
    "# Load Distillation Model\n",
    "model_ckpt = './distilbert-base-uncased-finetuned-clinc/checkpoint-500'\n",
    "tokenizer_distil = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model_distil = AutoModelForSequenceClassification.from_pretrained(model_ckpt).to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original Model test\n",
    "pipe_original = pipeline(\"text-classification\", model=model_original, tokenizer=tokenizer_original)\n",
    "optim_type = \"original\"\n",
    "# pb = PerformanceBenchmark(pipe_original, clinc[\"test\"], optim_type)\n",
    "# perf_metrics = pb.run_benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distillation Model test\n",
    "pipe_distil = pipeline(\"text-classification\", model=model_distil, tokenizer=tokenizer_distil)\n",
    "optim_type = \"distil\"\n",
    "# pb = PerformanceBenchmark(pipe_distil, clinc[\"test\"], optim_type)\n",
    "# perf_metrics.update(pb.run_benchmark())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantized\n",
    "from torch.quantization import quantize_dynamic\n",
    "\n",
    "model_quantized = quantize_dynamic(model_original, {nn.Linear}, dtype=torch.qint8)\n",
    "pipe = pipeline(\"text-classification\", model=model_quantized, tokenizer=tokenizer)\n",
    "optim_type = \"quantized\"\n",
    "# pb = PerformanceBenchmark(pipe, clinc[\"test\"], optim_type)\n",
    "# perf_metrics.update(pb.run_benchmark())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distilation + Quantized\n",
    "from torch.quantization import quantize_dynamic\n",
    "\n",
    "model_quantized2 = quantize_dynamic(model_distil, {nn.Linear}, dtype=torch.qint8)\n",
    "pipe = pipeline(\"text-classification\", model=model_quantized2, tokenizer=tokenizer)\n",
    "optim_type = \"distil+quantized\"\n",
    "# pb = PerformanceBenchmark(pipe, clinc[\"test\"], optim_type)\n",
    "# perf_metrics.update(pb.run_benchmark())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from psutil import cpu_count\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = str(cpu_count())\n",
    "os.environ[\"OMP_WAIT_POLICY\"] = \"ACTIVE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.convert_graph_to_onnx import convert\n",
    "from pathlib import Path\n",
    "\n",
    "onnx_model_path = Path(\"onnx/model.onnx\")\n",
    "convert(framework=\"pt\", \n",
    "        model=model_distil, # model의 경로(local/hub) 혹은 로드한 모델\n",
    "        tokenizer=tokenizer, \n",
    "        output=onnx_model_path, \n",
    "        opset=17, # onnx 라이브러리의 특정 버전\n",
    "        pipeline_name='text-classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime import GraphOptimizationLevel, InferenceSession, SessionOptions\n",
    "\n",
    "def create_model_for_provider(model_path, provider=[\"CPUExecutionProvider\"]):\n",
    "    options = SessionOptions()\n",
    "    options.intra_op_num_threads = 32\n",
    "    options.graph_optimization_level = GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "    \n",
    "    session = InferenceSession(str(model_path), options, providers=provider)\n",
    "    session.disable_fallback()\n",
    "    \n",
    "    return session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "class OnnxPipeline:\n",
    "    def __init__(self, model, tokenizer) -> None:\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __call__(self, query) -> list:\n",
    "        model_inputs = self.tokenizer(query, return_tensors=\"pt\")\n",
    "        inputs_onnx = {k: v.cpu().detach().numpy() for k, v in model_inputs.items()}\n",
    "        logits = self.model.run(None, inputs_onnx)[0][0,:]\n",
    "        probs = softmax(logits)\n",
    "        pred_idx = np.argmax(probs).item()\n",
    "        \n",
    "        return [{\"label\":intents.int2str(pred_idx), \"score\":probs[pred_idx]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OnnxPerformanceBenchmark(PerformanceBenchmark):\n",
    "    def __init__(self, *args, model_path, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.model_path = model_path\n",
    "    \n",
    "    def compute_size(self):\n",
    "        size_mb = Path(self.model_path).stat().st_size / 1024 / 1024\n",
    "        print(f\"Model size: {size_mb:.2f} MB\")\n",
    "        return {\"size_mb\": size_mb}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provider = [\n",
    "    ('CUDAExecutionProvider', {\n",
    "        'device_id': 0,\n",
    "        'arena_extend_strategy': 'kNextPowerOfTwo',\n",
    "        'gpu_mem_limit': 16 * 1024 * 1024 * 1024,\n",
    "        'cudnn_conv_algo_search': 'EXHAUSTIVE',\n",
    "        'do_copy_in_default_stream': True,\n",
    "    }),\n",
    "    'CPUExecutionProvider',\n",
    "]\n",
    "\n",
    "onnx_model = create_model_for_provider(onnx_model_path, provider=provider)\n",
    "onnx_pipe = OnnxPipeline(onnx_model, tokenizer)\n",
    "\n",
    "optim_type = \"Distil + ORT\"\n",
    "pb = OnnxPerformanceBenchmark(onnx_pipe, clinc['test'], optim_type, model_path='onnx/model.onnx')\n",
    "perf_metrics = pb.run_benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provider = ['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
    "\n",
    "onnx_model = create_model_for_provider(onnx_model_path, provider=provider)\n",
    "onnx_pipe = OnnxPipeline(onnx_model, tokenizer)\n",
    "\n",
    "optim_type = \"Distil + ORT\"\n",
    "pb = OnnxPerformanceBenchmark(onnx_pipe, clinc['test'], optim_type, model_path='onnx/model.onnx')\n",
    "perf_metrics = pb.run_benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model = create_model_for_provider(onnx_model_path, provider=['CPUExecutionProvider'])\n",
    "onnx_pipe = OnnxPipeline(onnx_model, tokenizer)\n",
    "\n",
    "optim_type = \"Distil + ORT\"\n",
    "pb = OnnxPerformanceBenchmark(onnx_pipe, clinc['test'], optim_type, model_path='onnx/model.onnx')\n",
    "perf_metrics.update(pb.run_benchmark())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "onnx_model = \"onnx/model.onnx\"\n",
    "quant_onnx_model = \"onnx/model.quant.onnx\"\n",
    "\n",
    "quantize_dynamic(model_input=onnx_model, model_output=quant_onnx_model, weight_type=QuantType.QInt8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_quantized_model = create_model_for_provider(quant_onnx_model)\n",
    "pipe = OnnxPipeline(onnx_quantized_model, tokenizer)\n",
    "optim_type = \"Distil+ORT(with Quantized)\"\n",
    "\n",
    "pb = OnnxPerformanceBenchmark(pipe, clinc[\"test\"], optim_type, model_path=quant_onnx_model)\n",
    "perf_metrics.update(pb.run_benchmark())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provider = ['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
    "\n",
    "onnx_quantized_model = create_model_for_provider(quant_onnx_model, provider=provider)\n",
    "pipe = OnnxPipeline(onnx_quantized_model, tokenizer)\n",
    "optim_type = \"Distil+ORT(with Quantized)\"\n",
    "\n",
    "pb = OnnxPerformanceBenchmark(pipe, clinc[\"test\"], optim_type, model_path=quant_onnx_model)\n",
    "perf_metrics.update(pb.run_benchmark())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provider = [\n",
    "    ('CUDAExecutionProvider', {\n",
    "        'device_id': 0,\n",
    "        'arena_extend_strategy': 'kNextPowerOfTwo',\n",
    "        'gpu_mem_limit': 16 * 1024 * 1024 * 1024,\n",
    "        'cudnn_conv_algo_search': 'EXHAUSTIVE',\n",
    "        'do_copy_in_default_stream': True,\n",
    "    }),\n",
    "    'CPUExecutionProvider',\n",
    "]\n",
    "\n",
    "onnx_quantized_model = create_model_for_provider(quant_onnx_model, provider=provider)\n",
    "pipe = OnnxPipeline(onnx_quantized_model, tokenizer)\n",
    "optim_type = \"Distil+ORT(with Quantized)\"\n",
    "\n",
    "pb = OnnxPerformanceBenchmark(pipe, clinc[\"test\"], optim_type, model_path=quant_onnx_model)\n",
    "perf_metrics.update(pb.run_benchmark())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "from pathlib import Path\n",
    "from time import perf_counter\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "\n",
    "clinc = load_dataset(\"clinc_oos\", \"plus\")\n",
    "intents = clinc[\"test\"].features[\"intent\"]\n",
    "accuracy_score = load_metric(\"accuracy\")\n",
    "\n",
    "class PerformanceBenchmark :\n",
    "    def __init__(self, pipeline, dataset, optim_type=\"BERT baseline\") :\n",
    "        self.pipeline = pipeline\n",
    "        self.dataset = dataset\n",
    "        self.optim_type = optim_type\n",
    "        \n",
    "    def compute_accuracy(self) :\n",
    "        preds, labels = [], []\n",
    "        for example in self.dataset :\n",
    "            pred = self.pipeline(example[\"text\"])[0][\"label\"]\n",
    "            label = example[\"intent\"]\n",
    "            preds.append(intents.str2int(pred))\n",
    "            labels.append(label)\n",
    "        accuracy = accuracy_score.compute(predictions=preds, references=labels)\n",
    "        print(f\"valid accuracy : {accuracy['accuracy']:.4f}\")\n",
    "        return accuracy\n",
    "    \n",
    "    def compute_size(self) :\n",
    "        state_dict = self.pipeline.model.state_dict()\n",
    "        tmp_path = Path(\"model.pt\")\n",
    "        torch.save(state_dict, tmp_path)\n",
    "        size_mb = Path(tmp_path).stat().st_size / (1024*1024)\n",
    "        tmp_path.unlink()\n",
    "        print(f\"model size : {size_mb:.4f} MB\")\n",
    "        return {\"size_mb\": size_mb}\n",
    "            \n",
    "    def time_pipeline(self, query=\"What is the pin number for my account?\") :\n",
    "        latencies = []\n",
    "        for _ in range(10) :\n",
    "            _ = self.pipeline(query)\n",
    "        for _ in range(100) :\n",
    "            start_time = perf_counter()\n",
    "            _ = self.pipeline(query)\n",
    "            latency = perf_counter() - start_time\n",
    "            latencies.append(latency)\n",
    "        time_avg_ms = 1000 * np.mean(latencies)\n",
    "        time_std_ms = 1000 * np.std(latencies)\n",
    "        print(f\"time avg : {time_avg_ms:.4f} ms +\\- {time_std_ms:.4f} ms\")\n",
    "        return {\"time_avg_ms\": time_avg_ms, \"time_std_ms\": time_std_ms}    \n",
    "    \n",
    "    def run_benchmark(self) :\n",
    "        metrics = {}\n",
    "        metrics[self.optim_type] = self.compute_size()\n",
    "        metrics[self.optim_type].update(self.time_pipeline())\n",
    "        metrics[self.optim_type].update(self.compute_accuracy())\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "# Original\n",
    "model_ckpt = 'transformersbook/bert-base-uncased-finetuned-clinc'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_ckpt).to('cuda')\n",
    "\n",
    "pipe = pipeline(\"text-classification\", model=model, tokenizer=tokenizer, device=0)\n",
    "optim_type = \"original-BERT\"\n",
    "pb = PerformanceBenchmark(pipe, clinc[\"test\"], optim_type)\n",
    "perf_metrics = pb.run_benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distilation Model & tokenizer\n",
    "\n",
    "model_ckpt = './distilbert-base-uncased-finetuned-clinc/checkpoint-500'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_ckpt)\n",
    "\n",
    "pipe = pipeline(\"text-classification\", \n",
    "                model=model, \n",
    "                tokenizer=tokenizer, \n",
    "                device=0)\n",
    "optim_type = \"distilBert\"\n",
    "pb = PerformanceBenchmark(pipe, clinc[\"test\"], optim_type)\n",
    "perf_metrics = pb.run_bnechmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Original Model & Tokenizer\n",
    "model_ckpt = 'transformersbook/bert-base-uncased-finetuned-clinc'\n",
    "tokenizer_original = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model_original = AutoModelForSequenceClassification.from_pretrained(model_ckpt).to('cuda')\n",
    "\n",
    "# Load Distillation Model\n",
    "model_ckpt = './distilbert-base-uncased-finetuned-clinc/checkpoint-500'\n",
    "tokenizer_distil = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model_distil = AutoModelForSequenceClassification.from_pretrained(model_ckpt).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original Model test\n",
    "pipe_original = pipeline(\"text-classification\", model=model_original, tokenizer=tokenizer_original, device=0)\n",
    "optim_type = \"original\"\n",
    "pb = PerformanceBenchmark(pipe_original, clinc[\"test\"], optim_type)\n",
    "perf_metrics = pb.run_benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distillation Model test\n",
    "pipe_distil = pipeline(\"text-classification\", model=model_distil, tokenizer=tokenizer_distil, device=0)\n",
    "optim_type = \"distil\"\n",
    "pb = PerformanceBenchmark(pipe_distil, clinc[\"test\"], optim_type)\n",
    "perf_metrics.update(pb.run_benchmark())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantized\n",
    "from torch.quantization import quantize_dynamic\n",
    "\n",
    "model_quantized = quantize_dynamic(model_original, dtype=torch.qint8)\n",
    "pipe = pipeline(\"text-classification\", model=model_quantized, tokenizer=tokenizer, device=0)\n",
    "optim_type = \"quantized\"\n",
    "pb = PerformanceBenchmark(pipe, clinc[\"test\"], optim_type)\n",
    "perf_metrics.update(pb.run_benchmark())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distilation + Quantized\n",
    "from torch.quantization import quantize_dynamic\n",
    "\n",
    "model_quantized2 = quantize_dynamic(model_distil, {nn.Linear}, dtype=torch.qint8)\n",
    "pipe = pipeline(\"text-classification\", model=model_quantized2, tokenizer=tokenizer, device=0)\n",
    "optim_type = \"distil+quantized\"\n",
    "pb = PerformanceBenchmark(pipe, clinc[\"test\"], optim_type)\n",
    "perf_metrics.update(pb.run_benchmark())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
